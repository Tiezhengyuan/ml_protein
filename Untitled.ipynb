{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "887dd655-074b-41c6-b6cb-0b284af59ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_in_notebook() -> bool:\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/a/39662359\\\n",
    "    It returns 'TerminalInteractiveShell' on a terminal IPython,\n",
    "    'ZMQInteractiveShell' on Jupyter (notebook AND qtconsole) and fails\n",
    "    (NameError) on a regular Python interpreter. The method get_python() seems\n",
    "    to be available in the global namespace\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return get_ipython().__class__.__name__ == \"ZMQInteractiveShell\"\n",
    "    except NameError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d679f338-9760-41fd-8b04-2db6d77c4511",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab_black'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pip install git+https://github.com/n8henrie/nb_black\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlab_black\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m running_in_notebook():\n\u001b[1;32m      5\u001b[0m     lab_black\u001b[38;5;241m.\u001b[39mload_ipython_extension(get_ipython(), line_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m79\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab_black'"
     ]
    }
   ],
   "source": [
    "# pip install git+https://github.com/n8henrie/nb_black\n",
    "import lab_black\n",
    "\n",
    "if running_in_notebook():\n",
    "    lab_black.load_ipython_extension(get_ipython(), line_length=79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a979bde3-4209-4383-9769-bdc5f4ce24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchtext\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb3c6c8-b94e-4f27-86c6-20ba9c63d74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.3.1+cu121', '0.18.0+cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80e02ba0-9c5c-43a1-8bda-f9686d336f37",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[1;32m      2\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m----> 5\u001b[0m TEXT \u001b[38;5;241m=\u001b[39m torchtext\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mField(\n\u001b[1;32m      6\u001b[0m     lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, include_lengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m LABEL \u001b[38;5;241m=\u001b[39m torchtext\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mField(sequential\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m train_txt, test_txt \u001b[38;5;241m=\u001b[39m torchtext\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mIMDB\u001b[38;5;241m.\u001b[39msplits(TEXT, LABEL)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchtext' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "max_length = 256\n",
    "\n",
    "\n",
    "TEXT = torchtext.data.Field(\n",
    "    lower=True, include_lengths=False, batch_first=True\n",
    ")\n",
    "LABEL = torchtext.data.Field(sequential=False)\n",
    "\n",
    "train_txt, test_txt = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "TEXT.build_vocab(\n",
    "    train_txt,\n",
    "    vectors=torchtext.vocab.GloVe(name=\"6B\", dim=50, max_vectors=50_000),\n",
    "    max_size=50_000,\n",
    ")\n",
    "\n",
    "LABEL.build_vocab(train_txt)\n",
    "\n",
    "train_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train_txt, test_txt),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f95355e7-9002-409a-bed1-a4dd5f25d37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_CACHE_DIR',\n",
       " '_TEXT_BUCKET',\n",
       " '_TORCHTEXT_DEPRECATION_MSG',\n",
       " '_WARN',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_extension',\n",
       " '_get_torch_home',\n",
       " '_internal',\n",
       " '_torchtext',\n",
       " 'disable_torchtext_deprecation_warning',\n",
       " 'git_version',\n",
       " 'os',\n",
       " 'version']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torchtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72846e30-7506-4392-bff6-0ff736e3ea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuan/anaconda3/envs/py311/lib/python3.11/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/yuan/anaconda3/envs/py311/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Package `torchdata` not found. Please install following instructions at https://github.com/pytorch/data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import datasets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMDB\n\u001b[0;32m----> 4\u001b[0m train_iter \u001b[38;5;241m=\u001b[39m IMDB(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(label, line):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m line\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torchtext/data/datasets_utils.py:193\u001b[0m, in \u001b[0;36m_create_dataset_directory.<locals>.decorator.<locals>.wrapper\u001b[0;34m(root, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(new_root):\n\u001b[1;32m    192\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(new_root, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(root\u001b[38;5;241m=\u001b[39mnew_root, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torchtext/data/datasets_utils.py:155\u001b[0m, in \u001b[0;36m_wrap_split_argument_with_fn.<locals>.new_fn\u001b[0;34m(root, split, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m _check_default_set(split, splits, fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(fn(root, item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_datasets(\u001b[38;5;28mtuple\u001b[39m(result), split)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torchtext/datasets/imdb.py:87\u001b[0m, in \u001b[0;36mIMDB\u001b[0;34m(root, split)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"IMDB Dataset\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m.. warning::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m:rtype: (int, str)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage `torchdata` not found. Please install following instructions at https://github.com/pytorch/data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileOpener, GDriveReader, HttpReader, IterableWrapper  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     92\u001b[0m url_dp \u001b[38;5;241m=\u001b[39m IterableWrapper([URL])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Package `torchdata` not found. Please install following instructions at https://github.com/pytorch/data"
     ]
    }
   ],
   "source": [
    "# import datasets\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "def tokenize(label, line):\n",
    "    return line.split()\n",
    "\n",
    "tokens = []\n",
    "for label, line in train_iter:\n",
    "    tokens += tokenize(label, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbbbee-2bf0-4c1c-a420-543bcf6d0062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969f8e0-2843-40fe-9c88-b8701ca779f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
